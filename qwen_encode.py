import math
import torch
import comfy.utils
import node_helpers

# ========================================================
# è¾…åŠ©å‡½æ•°ï¼šæ£€æµ‹æ˜¯å¦ä¸ºæ— æ•ˆå ä½å›¾
# ========================================================
def is_valid_image(img):
    """
    åˆ¤æ–­å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆã€‚
    å¦‚æœå›¾ç‰‡æ˜¯ Noneï¼Œæˆ–è€…æ£€æµ‹åˆ°æ˜¯çº¯é»‘/çº¯ç™½çš„å ä½å›¾ï¼Œè¿”å› Falseã€‚
    """
    if img is None:
        return False
    
    # æ£€æŸ¥ Tensor æ˜¯å¦ä¸ºç©º
    if img.numel() == 0:
        return False

    # æ€§èƒ½ä¼˜åŒ–ï¼šåªæ£€æŸ¥æå€¼ã€‚
    # Matrix èŠ‚ç‚¹ç”Ÿæˆçš„å ä½å›¾æ˜¯çº¯ 0.0 (é»‘) æˆ–çº¯ 1.0 (ç™½)ã€‚
    # å¦‚æœ min == maxï¼Œè¯´æ˜æ•´å¼ å›¾åªæœ‰ä¸€ä¸ªé¢œè‰²ã€‚
    # å¹¶ä¸”è¿™ä¸ªé¢œè‰²æ˜¯ 0 æˆ– 1ï¼Œé‚£å¤§æ¦‚ç‡å°±æ˜¯å ä½å›¾ã€‚
    min_val = img.min().item()
    max_val = img.max().item()
    
    if min_val == max_val:
        if min_val == 0.0 or min_val == 1.0:
            return False
            
    return True

# ========================================================
# èŠ‚ç‚¹ 1: 5å›¾æ ‡å‡†ç‰ˆ (Strict Original + Smart Filter)
# ========================================================
class MatrixTextEncodeQwen5:
    """
    Qwen Text Encode (5 Images)
    1. å¢åŠ â€œæ™ºèƒ½è¿‡æ»¤â€ï¼šè‡ªåŠ¨å¿½ç•¥çº¯é»‘/çº¯ç™½å ä½å›¾ã€‚
    2. å‚æ•°åä¿æŒ image1... å®˜æ–¹å…¼å®¹ã€‚
    """
    
    DESCRIPTION = """
    ã€Qwen-VL ç¼–ç å™¨ (5å›¾ç‰ˆ)ã€‘
    åŠŸèƒ½ï¼šä¸“ä¸º Qwen-VL æ¨¡å‹è®¾è®¡çš„æ–‡æœ¬+å›¾åƒç¼–ç èŠ‚ç‚¹ã€‚
    
    ğŸš€ æ™ºèƒ½ç‰¹æ€§ï¼š
    å†…ç½®â€œå ä½å›¾è¿‡æ»¤å™¨â€ã€‚å¦‚æœä½ è¿æ¥äº† Matrix Loader çš„ç©ºæ’æ§½ï¼ˆè¾“å‡ºçº¯é»‘/ç™½å›¾ï¼‰ï¼Œ
    æœ¬èŠ‚ç‚¹ä¼šè‡ªåŠ¨å°†å…¶å¿½ç•¥ï¼Œä¸è®¡å…¥ Tokenã€‚
    è¿™æ„å‘³ç€ä½ å¯ä»¥æ”¾å¿ƒåœ°æŠŠ 5 æ ¹çº¿å…¨è¿ä¸Šï¼Œåªç”¨å…¶ä¸­å‡ å¼ ï¼Œå®Œå…¨ä¸å½±å“æ•ˆæœï¼
    """

    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "clip": ("CLIP", ),
            "prompt": ("STRING", {"multiline": True, "dynamicPrompts": True}),
            "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}), 
            "smart_input": ("BOOLEAN", {"default": False, "tooltip": "å¼€å¯åï¼Œæ ¹æ®ã€æœ‰æ•ˆå›¾ç‰‡ã€‘çš„æ•°é‡è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡ã€‚"}), 
            "align_latent": (["disabled", "image1_only", "all"], {"default": "image1_only"}), 
            },
            "optional": {
                "vae": ("VAE", ),
                "image1": ("IMAGE", ),
                "image2": ("IMAGE", ),
                "image3": ("IMAGE", ),
                "image4": ("IMAGE", ),
                "image5": ("IMAGE", ),
            }}
    
    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT",)
    RETURN_NAMES = ("cond+", "cond-", "latent")
    FUNCTION = "encode"
    
    CATEGORY = "Custom/Matrix"
    
    def encode(self, clip, prompt, negative_prompt, smart_input, align_latent, vae=None, image1=None, image2=None, image3=None, image4=None, image5=None):
        ref_latents = []
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ™ºèƒ½è¿‡æ»¤ ===
        raw_images = [image1, image2, image3, image4, image5]
        images = [img for img in raw_images if is_valid_image(img)]
        # ===========================
        
        images_vl = []
        llama_template = "<|im_start|>system\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
        image_prompt = ""
        output_latent = None
        
        size = 384
        if smart_input:
            size = 1024
            if len(images) > 2:
                size = 384
            elif len(images) > 1:
                size = 512
        
        for i, image in enumerate(images):
            samples = image.movedim(-1, 1)
            total = int(size * size)
            
            scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
            width = round(samples.shape[3] * scale_by)
            height = round(samples.shape[2] * scale_by)
            
            s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
            images_vl.append(s.movedim(1, -1))
            
            if vae is not None:
                if (align_latent == "image1_only" and i == 0) or align_latent == "all":
                    l = vae.encode(image[:, :, :, :3])
                    if i == 0:
                        output_latent = l
                    ref_latents.append(l)
                else:
                    if i == 0:
                        output_latent = vae.encode(image[:, :, :, :3])
                    total = int(1024 * 1024)
                    scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
                    width = round(samples.shape[3] * scale_by / 8.0) * 8
                    height = round(samples.shape[2] * scale_by / 8.0) * 8
                    
                    s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
                    ref_latents.append(vae.encode(s.movedim(1, -1)[:, :, :, :3]))
                
            image_prompt += "Picture {}: <|vision_start|><|image_pad|><|vision_end|>".format(i + 1)
                
        tokens = clip.tokenize(image_prompt + prompt, images=images_vl, llama_template=llama_template)
        conditioning = clip.encode_from_tokens_scheduled(tokens)
        tokensN = clip.tokenize(image_prompt + negative_prompt, images=images_vl, llama_template=llama_template)
        conditioningN = clip.encode_from_tokens_scheduled(tokensN)
        
        if len(ref_latents) > 0:
            conditioning = node_helpers.conditioning_set_values(conditioning, {"reference_latents": ref_latents}, append=True)
            conditioningN = node_helpers.conditioning_set_values(conditioningN, {"reference_latents": ref_latents}, append=True)
        
        return (conditioning, conditioningN, {"samples": output_latent}, )

# ========================================================
# èŠ‚ç‚¹ 2: 10å›¾è¯•éªŒç‰ˆ (Strict Original + Smart Filter)
# ========================================================
class MatrixTextEncodeQwen10:
    """
    Qwen Text Encode (10 Images) - Experimental
    åŒæ ·å¢åŠ äº†æ™ºèƒ½è¿‡æ»¤ã€‚
    """
    
    DESCRIPTION = """
    ã€Qwen-VL ç¼–ç å™¨ (10å›¾è¯•éªŒç‰ˆ)ã€‘
    åŠŸèƒ½ï¼šæ‰©å±•äº†è¾“å…¥ä¸Šé™ã€‚
    æ™ºèƒ½ç‰¹æ€§ï¼šåŒæ ·å†…ç½®â€œå ä½å›¾è¿‡æ»¤å™¨â€ï¼Œè‡ªåŠ¨å‰”é™¤çº¯é»‘/çº¯ç™½å›¾ç‰‡ï¼Œå‡å°‘æ¨¡å‹å¹²æ‰°ã€‚
    """

    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "clip": ("CLIP", ),
            "prompt": ("STRING", {"multiline": True, "dynamicPrompts": True}),
            "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}), 
            "smart_input": ("BOOLEAN", {"default": False}), 
            "align_latent": (["disabled", "image1_only", "all"], {"default": "image1_only"}), 
            },
            "optional": {
                "vae": ("VAE", ),
                "image1": ("IMAGE", ), "image2": ("IMAGE", ), "image3": ("IMAGE", ), "image4": ("IMAGE", ), "image5": ("IMAGE", ),
                "image6": ("IMAGE", ), "image7": ("IMAGE", ), "image8": ("IMAGE", ), "image9": ("IMAGE", ), "image10": ("IMAGE", ),
            }}
    
    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT",)
    RETURN_NAMES = ("cond+", "cond-", "latent") 
    FUNCTION = "encode"
    
    CATEGORY = "Custom/Matrix"
    
    def encode(self, clip, prompt, negative_prompt, smart_input, align_latent, vae=None, image1=None, image2=None, image3=None, image4=None, image5=None, image6=None, image7=None, image8=None, image9=None, image10=None):
        ref_latents = []
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ™ºèƒ½è¿‡æ»¤ ===
        raw_images = [image1, image2, image3, image4, image5, image6, image7, image8, image9, image10]
        images = [img for img in raw_images if is_valid_image(img)]
        # ===========================
        
        images_vl = []
        llama_template = "<|im_start|>system\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
        image_prompt = ""
        output_latent = None
        
        size = 384
        if smart_input:
            size = 1024
            if len(images) > 2:
                size = 384
            elif len(images) > 1:
                size = 512
        
        for i, image in enumerate(images):
            samples = image.movedim(-1, 1)
            total = int(size * size)
            
            scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
            width = round(samples.shape[3] * scale_by)
            height = round(samples.shape[2] * scale_by)
            
            s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
            images_vl.append(s.movedim(1, -1))
            
            if vae is not None:
                if (align_latent == "image1_only" and i == 0) or align_latent == "all":
                    l = vae.encode(image[:, :, :, :3])
                    if i == 0:
                        output_latent = l
                    ref_latents.append(l)
                else:
                    if i == 0:
                        output_latent = vae.encode(image[:, :, :, :3])
                    total = int(1024 * 1024)
                    scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
                    width = round(samples.shape[3] * scale_by / 8.0) * 8
                    height = round(samples.shape[2] * scale_by / 8.0) * 8
                    
                    s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
                    ref_latents.append(vae.encode(s.movedim(1, -1)[:, :, :, :3]))
                
            image_prompt += "Picture {}: <|vision_start|><|image_pad|><|vision_end|>".format(i + 1)
                
        tokens = clip.tokenize(image_prompt + prompt, images=images_vl, llama_template=llama_template)
        conditioning = clip.encode_from_tokens_scheduled(tokens)
        tokensN = clip.tokenize(image_prompt + negative_prompt, images=images_vl, llama_template=llama_template)
        conditioningN = clip.encode_from_tokens_scheduled(tokensN)
        
        if len(ref_latents) > 0:
            conditioning = node_helpers.conditioning_set_values(conditioning, {"reference_latents": ref_latents}, append=True)
            conditioningN = node_helpers.conditioning_set_values(conditioningN, {"reference_latents": ref_latents}, append=True)
        
        return (conditioning, conditioningN, {"samples": output_latent}, )

NODE_CLASS_MAPPINGS = {
    "MatrixTextEncodeQwen5": MatrixTextEncodeQwen5,
    "MatrixTextEncodeQwen10": MatrixTextEncodeQwen10
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MatrixTextEncodeQwen5": "Matrix Qwen Encode (5)",
    "MatrixTextEncodeQwen10": "Matrix Qwen Encode (10 Experimental)"
}