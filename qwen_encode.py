import math
import torch
import comfy.utils
import node_helpers

def is_valid_image(img):
    if img is None: return False
    if img.numel() == 0: return False
    min_val = img.min().item()
    max_val = img.max().item()
    if min_val == max_val and (min_val == 0.0 or min_val == 1.0):
        return False
    return True

# ========================================================
# èŠ‚ç‚¹ 1: 5å›¾æ ‡å‡†ç‰ˆ
# ========================================================
class MatrixTextEncodeQwen5:
    """
    Qwen Text Encode (5 Images)
    ğŸš€ æ ¸å¿ƒå‡çº§ï¼šåŠ¨æ€é‡æ’é€»è¾‘
    ä¸ç®¡ä½ é€‰ä¸­å“ªå¼ å›¾åš Alignï¼Œæœ¬èŠ‚ç‚¹éƒ½ä¼šæŠŠå®ƒå·å·æŒªåˆ° Picture 1 çš„ä½ç½®é€ç»™æ¨¡å‹ã€‚
    è¿™èƒ½å®Œç¾è§£å†³â€œåªæœ‰ Image 1 èƒ½å¯¹é½â€çš„é—®é¢˜ã€‚
    """
    
    DESCRIPTION = """
    ã€Qwen-VL ç¼–ç å™¨ (5å›¾ç‰ˆ)ã€‘
    ğŸš€ æ™ºèƒ½é‡æ’æŠ€æœ¯ï¼š
    æ— è®ºä½ é€‰æ‹© Image 3 è¿˜æ˜¯ Image 5 ä½œä¸ºå¯¹é½åº•æ¿ï¼Œ
    æœ¬èŠ‚ç‚¹éƒ½ä¼šè‡ªåŠ¨å°†å…¶è°ƒæ•´ä¸ºæ¨¡å‹çœ¼ä¸­çš„â€œç¬¬ä¸€å¼ å›¾â€ã€‚
    å½»åº•è§£å†³é Image 1 æ— æ³•å¯¹é½çš„ç—›ç‚¹ï¼
    """

    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "clip": ("CLIP", ),
            "prompt": ("STRING", {"multiline": True, "dynamicPrompts": True}),
            "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}), 
            "smart_input": ("BOOLEAN", {"default": False}), 
            "align_latent": (["disabled", "image1", "image2", "image3", "image4", "image5"], {"default": "image1"}), 
            },
            "optional": {
                "vae": ("VAE", ),
                "image1": ("IMAGE", ),
                "image2": ("IMAGE", ),
                "image3": ("IMAGE", ),
                "image4": ("IMAGE", ),
                "image5": ("IMAGE", ),
            }}
    
    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT",)
    RETURN_NAMES = ("cond+", "cond-", "latent")
    FUNCTION = "encode"
    
    CATEGORY = "Custom/Matrix"
    
    def encode(self, clip, prompt, negative_prompt, smart_input, align_latent, vae=None, image1=None, image2=None, image3=None, image4=None, image5=None):
        raw_inputs = [image1, image2, image3, image4, image5]
        
        # 1. ç¡®å®šè°æ˜¯ä¸»è§’ (Align Target)
        target_img = None
        other_images = []
        
        target_idx = -1
        if align_latent != "disabled":
            try:
                target_idx = int(align_latent.replace("image", "")) - 1
            except: pass

        # 2. æ„å»ºé‡æ’åçš„åˆ—è¡¨ (valid_images)
        # é€»è¾‘ï¼šå¦‚æœæŒ‡å®šäº† Target ä¸”æœ‰æ•ˆï¼ŒæŠŠå®ƒæ”¾åˆ°åˆ—è¡¨ç¬¬ä¸€ä½ (index 0)
        # å…¶ä»–æœ‰æ•ˆå›¾ç‰‡è·Ÿåœ¨åé¢
        
        for idx, img in enumerate(raw_inputs):
            if is_valid_image(img):
                if idx == target_idx:
                    target_img = img # æ‰¾åˆ°ä¸»è§’äº†
                else:
                    other_images.append(img) # é…è§’å…ˆæ’é˜Ÿ
        
        final_images = []
        output_latent = None
        
        if target_img is not None:
            # ä¸»è§’æ’é˜Ÿåˆ°ç¬¬ä¸€ä½ï¼
            final_images.append(target_img)
            # è®¡ç®— Latent
            if vae is not None:
                output_latent = vae.encode(target_img[:, :, :, :3])
        
        # æŠŠå…¶ä»–é…è§’æ¥åœ¨åé¢
        final_images.extend(other_images)
        
        # 3. å¼€å§‹ç¼–ç  (æ­¤æ—¶ final_images[0] ä¸€å®šæ˜¯æˆ‘ä»¬è¦å¯¹é½çš„é‚£å¼ å›¾)
        ref_latents = []
        images_vl = []
        llama_template = "<|im_start|>system\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
        image_prompt = ""
        
        size = 384
        if smart_input:
            size = 1024
            if len(final_images) > 2:
                size = 384
            elif len(final_images) > 1:
                size = 512
        
        for i, image in enumerate(final_images):
            samples = image.movedim(-1, 1)
            total = int(size * size)
            scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
            width = round(samples.shape[3] * scale_by)
            height = round(samples.shape[2] * scale_by)
            s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
            images_vl.append(s.movedim(1, -1))
            
            if vae is not None:
                l = vae.encode(image[:, :, :, :3])
                ref_latents.append(l)
                
            image_prompt += "Picture {}: <|vision_start|><|image_pad|><|vision_end|>".format(i + 1)
                
        tokens = clip.tokenize(image_prompt + prompt, images=images_vl, llama_template=llama_template)
        conditioning = clip.encode_from_tokens_scheduled(tokens)
        tokensN = clip.tokenize(image_prompt + negative_prompt, images=images_vl, llama_template=llama_template)
        conditioningN = clip.encode_from_tokens_scheduled(tokensN)
        
        if len(ref_latents) > 0:
            conditioning = node_helpers.conditioning_set_values(conditioning, {"reference_latents": ref_latents}, append=True)
            conditioningN = node_helpers.conditioning_set_values(conditioningN, {"reference_latents": ref_latents}, append=True)
        
        return (conditioning, conditioningN, {"samples": output_latent}, )

# ========================================================
# èŠ‚ç‚¹ 2: 10å›¾è¯•éªŒç‰ˆ
# ========================================================
class MatrixTextEncodeQwen10:
    """
    Qwen Text Encode (10 Images) - Experimental
    åŒæ ·çš„é‡æ’é€»è¾‘ã€‚
    """
    
    DESCRIPTION = """
    ã€Qwen-VL ç¼–ç å™¨ (10å›¾è¯•éªŒç‰ˆ)ã€‘
    åŠŸèƒ½ï¼šæ‰©å±•äº†è¾“å…¥ä¸Šé™ï¼Œæ”¯æŒè‡ªç”±é€‰æ‹© 1-10 ä»»æ„ä¸€å¼ ä½œä¸ºåº•å›¾ï¼ˆè‡ªåŠ¨é‡æ’åˆ°ç¬¬ä¸€ä½ï¼‰ã€‚
    """

    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "clip": ("CLIP", ),
            "prompt": ("STRING", {"multiline": True, "dynamicPrompts": True}),
            "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}), 
            "smart_input": ("BOOLEAN", {"default": False}), 
            "align_latent": (["disabled", "image1", "image2", "image3", "image4", "image5", "image6", "image7", "image8", "image9", "image10"], {"default": "image1"}), 
            },
            "optional": {
                "vae": ("VAE", ),
                "image1": ("IMAGE", ), "image2": ("IMAGE", ), "image3": ("IMAGE", ), "image4": ("IMAGE", ), "image5": ("IMAGE", ),
                "image6": ("IMAGE", ), "image7": ("IMAGE", ), "image8": ("IMAGE", ), "image9": ("IMAGE", ), "image10": ("IMAGE", ),
            }}
    
    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT",)
    RETURN_NAMES = ("cond+", "cond-", "latent") 
    FUNCTION = "encode"
    
    CATEGORY = "Custom/Matrix"
    
    def encode(self, clip, prompt, negative_prompt, smart_input, align_latent, vae=None, image1=None, image2=None, image3=None, image4=None, image5=None, image6=None, image7=None, image8=None, image9=None, image10=None):
        raw_inputs = [image1, image2, image3, image4, image5, image6, image7, image8, image9, image10]
        
        target_img = None
        other_images = []
        target_idx = -1
        
        if align_latent != "disabled":
            try:
                target_idx = int(align_latent.replace("image", "")) - 1
            except: pass

        for idx, img in enumerate(raw_inputs):
            if is_valid_image(img):
                if idx == target_idx:
                    target_img = img
                else:
                    other_images.append(img)
        
        final_images = []
        output_latent = None
        
        if target_img is not None:
            final_images.append(target_img)
            if vae is not None:
                output_latent = vae.encode(target_img[:, :, :, :3])
        
        final_images.extend(other_images)
        
        ref_latents = []
        images_vl = []
        llama_template = "<|im_start|>system\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
        image_prompt = ""
        
        size = 384
        if smart_input:
            size = 1024
            if len(final_images) > 2:
                size = 384
            elif len(final_images) > 1:
                size = 512
        
        for i, image in enumerate(final_images):
            samples = image.movedim(-1, 1)
            total = int(size * size)
            scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))
            width = round(samples.shape[3] * scale_by)
            height = round(samples.shape[2] * scale_by)
            s = comfy.utils.common_upscale(samples, width, height, "area", "disabled")
            images_vl.append(s.movedim(1, -1))
            
            if vae is not None:
                l = vae.encode(image[:, :, :, :3])
                ref_latents.append(l)
            image_prompt += "Picture {}: <|vision_start|><|image_pad|><|vision_end|>".format(i + 1)
                
        tokens = clip.tokenize(image_prompt + prompt, images=images_vl, llama_template=llama_template)
        conditioning = clip.encode_from_tokens_scheduled(tokens)
        tokensN = clip.tokenize(image_prompt + negative_prompt, images=images_vl, llama_template=llama_template)
        conditioningN = clip.encode_from_tokens_scheduled(tokensN)
        
        if len(ref_latents) > 0:
            conditioning = node_helpers.conditioning_set_values(conditioning, {"reference_latents": ref_latents}, append=True)
            conditioningN = node_helpers.conditioning_set_values(conditioningN, {"reference_latents": ref_latents}, append=True)
        
        return (conditioning, conditioningN, {"samples": output_latent}, )

NODE_CLASS_MAPPINGS = {
    "MatrixTextEncodeQwen5": MatrixTextEncodeQwen5,
    "MatrixTextEncodeQwen10": MatrixTextEncodeQwen10
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MatrixTextEncodeQwen5": "Matrix Qwen Encode (5)",
    "MatrixTextEncodeQwen10": "Matrix Qwen Encode (10 Experimental)"
}